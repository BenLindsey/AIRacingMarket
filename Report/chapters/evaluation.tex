\section{Personas}
As students pursuing industrial placements and graduate opportunities, we have a good understanding of our users - students and recruiters. We formalised this understanding through two key personas: Nicolas, a Computing student interested in graduate opportunities and Joshua, a software developer and recruiter at G-Research.
\subsection{Joshua}
\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{moon.png}
\end{minipage}%
\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{joshua.png}
\end{minipage}
\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{dragonboat.png}
\end{minipage}\\

Joshua has worked full-time for G-Research for 3 years now, and has been actively involved in graduate recruitment since working as a student ambassador following a summer internship. Joshua works as a Machine Learning Analyst, creating financial models to predict investment returns. Many of Joshua's colleagues have postgraduate degrees, however he recognises that mathematics and computing undergraduate students may have the required skills - in particular, the ability to independently implement theoretical ideas as working code. 

In his spare time, Joshua enjoys dragon boating along the Thames with friends from the office. Joshua also contributes to open source projects, for example a virtual moon atlas. Many students that Joshua meets at career fairs also pursue coding projects in their own time, however Joshua enjoys talking to any bright, enthusiastic and numerate student. Other than a few students that beeline to the G-Research stand, Joshua unfortunately finds it hard to attract attention, as most students flock to famous companies such as Google. When he does manage to attract a student's attention, he occasionally notices students queueing to talk with him, and then leaving before he gets the chance. 

\subsection{Nicolas}

\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{puzzle.png}
\end{minipage}%
\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{nicolas.png}
\end{minipage}
\begin{minipage}{.333\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{hackathon.png}
\end{minipage}\\

Nicolas is a 3rd year BEng Computing student looking for a graduate role as a developer. He has a lot of programming experience from his degree, so finds it easy to work with new languages and get his head around complicated concepts. He could easily understand a simple API in a few minutes if it is well documented and intuitive. He enjoys puzzles and challenges and takes part in hackathons (such as Facebook Campus hackathons).

He plans to attend the Technology Careers Fair to find companies he would like to work at and apply to. He will attend the stalls of various well known companies, such as Google and Facebook. He is less inclined to talk with smaller companies unless their stand has a specific draw. Nicolas doesn't want to commit to being at a stand for long time, any activity that takes longer than 10 minutes might dissuade him from visiting a company. 

\section{Value Proposition Canvas}
We used our personas to create a Value Proposition Canvas that summarizes our understanding of the customer, the product we are producing and the explicit ways it is creating value for both G-Research and the students who will use it. It is clear that the gains/pains and their respective solutions, detailed below, are essentially hypotheses. As we moved forwards with user testing it was interesting to evaluate these proposed gain creators and pain relievers, and pivot development accordingly. We discuss our hallway testing later in the chapter (section 5.5). 

\subsection{Customer Segment}
\begin{tabularx}{\textwidth}{| c | X | X | X |}
	\hline
	& Customer Job(s) & Gains & Pains \\ 
	\hline\hline
	\multirow{4}{*}{Recruiter} & Advertise G-Research at  university careers fairs or special events
		       & Potential for fast tracking candidates 
		       & Not attracting candidates to G-Research stand at careers fair \\ \cline{2-4}

		       & Identify prospective employees
		       & Standing out at careers fairs among other companies, i.e. brand exposure
		       & Attracting unsuitable candidates \\ \cline{2-4}
	
		       & 
		       & Better understanding of what candidates look for when it comes to internships
		       & Generating too much attention to manage with only a few recruiters present \\ \cline{3-3}

		       & 
		       & Survey data about candidates, e.g. degree/year group
		       & \\ 
	
	\hline\hline
	\multirow{4}{*}{Student} & Learn about G-Research and its internship/graduate opportunities
		       & Enjoyable and informative conversation with recruiter from G-Research
		       & Waiting or not getting a chance to talk to a recruiter \\ \cline{2-4}

		       & If they like G-Research, apply for internship/graduate opportunities
		       & Contact details or application referral from recruiter
		       & Not learning enough about G-Research \\ \cline{2-4}

			&
			& Way of expressing programming/problem solving ability and impressing recruiter
			& Awkward conversation with a G-Research recruiter \\ \cline{3-4}
	
			&
			& 
			& Getting too much unwanted attention from a recruiter \\ \cline{4-4}

			&
			& 
			&  Unwanted pressure - feeling like performance/conversation is pivotal to application\\
	\hline
\end{tabularx}

\subsection{Value Proposition}
We can summarise our proposed product/service, before analysing how it influences our customers : 
\\[+1em]
\begin{tabular}{ r | l}
{\bf AI Racing Market} & Interactive multiplayer racing game \\
				   & Users compete by creating AI scripts\\
				   & Online code editor with javascript syntax highlighting \\
				   & User profiles with optional university/degree information \\
				   & Edit and test previous scripts \\
				   & Leaderboard system
\end{tabular} 
\\[+2em]

\begin{tabularx}{\textwidth}{| c | X | X | X |}
\hline
& Gain Creators & Pain Relievers \\ 
\hline\hline
\multirow{4}{*}{Recruiter} 
	       & Candidates may become more relaxed and easier to talk to if enjoying the game
	       & Few recruiters go as far as having games at fairs, likely to attract interest \\ \cline{2-3}

	       & If candidates leave their email and produce good scripts, recruiters can get in contact with them
	       & Producing good AI requires logical problem solving, alongside programming skills which could be learnt by an inexperience but patient candidate - these candidates are likely appealing to G-Research\\ \cline{2-3}

	       & Recruiters can work with students and gave a genuine demonstration of how they work collaboratively/pair program at G-Research
	       & Large screen can show races, attracting spectators without forcing participation\\ 
\hline\hline
\multirow{4}{*}{Student} 
	       & Can demonstrate both programming and logical problem solving skills to recruiter.
	       & Option to anonymously submit AI scripts, so there is no concern over AI performance being tracker and harming G-Research application\\ \cline{2-3}

	       & Can demonstrate teamwork skills if collaborating with other candidates
	       & Can collaborate with other students whilst waiting for a turn\\ \cline{2-3}

	       & May have positive experience developing iteratively with G-Research recruiter
	       & Recruiters are more likely free to talk if some candidates are busy playing game\\ \cline{2-3}
	       
	       & G-Research may award prizes for particularly impressive candidates
	       & Use of familiar programming language ({\tt javascript}) which also shouldn't punish inexperienced coders\\ 
\hline
\end{tabularx}

\section{Customer Feedback and Evaluation}
The primary point of contact with our customer, Ed, was a fortnightly sprint review meeting. These took place at the end of each sprint, allowing us to discuss progress made in the previous sprint and any expectations for the next one. To illustrate this let's examine a previous meeting:
\begin{enumerate}
\item First of all we set up a demo for the latest build of our project. Keeping a clean master branch for our stable code is critical for this, since we wouldn't want to be running around bug hunting just before Ed arrived.
\item After welcoming Ed to college, we ran through our new features on the demo, highlighting any user stories we successfully implemented. 
\item We explained any scoping decisions we had taken, such as pushing back user stories to the next sprint or pulling them forward. We were not afraid to take full advantage of Ed's knowledge as a developer, knowing he could understand some of the technical reasons behind the choices we'd made. 
\item  After this we passed the baton over to Ed, allowing him to offer any feedback on the features we'd implemented and discuss if he accepted the user stories as complete or still in need of work. He also discussed how the features fit into his vision of the final product, i.e. if we were making progress or racing off track. This feedback served as a last resort for keeping our goals aligned with our customer, preferably we would know if the feature fit into his vision before we began implementing it.
\item Once the evaluation stage was satisfied we discussed next week's sprint, in particular the stories he wanted us to work on. We divided these into requirements and stretch goals by estimating the development time required and the importance of the feature. Having this divide between stretch goals and requirements allowed us to balance our time appropriately, making sure we were steadily iterating towards a solid product while also able to add exciting, if not critical, features.
\item Finally we reviewed our progress in regards to the long term picture, considering how far we had made it towards the final product. We considered any features we would likely need to cut or add and planned around this appropriately.
\end{enumerate}

However, limiting contact time to once every two weeks is a dangerous trap to fall into; it would have left little room for us to maneuver around changing customer requirements or our own busy schedules. To help encourage continuous feedback throughout the project, we added Ed to our Trello board. This allowed him to keep track of our progress in between face-to-face meetings without adding extra communication overhead on our side. 

As for our communication needs, we could contact Ed in light of developing problems directly via email. One of such issues that arose was what environment we could expect our project to be running in - early in the project we had little knowledge of what technologies G-Research would be willing to support, making design decisions troublesome. It was critical that we got feedback from Ed immediately about what licenses and tools he'd be willing to burden. Leaving such discussions to the next meeting would have wasted huge amounts of development time in waiting, or scrapping unsuitable work.

On the other hand there are some discussions we could only have with Ed in the room. Some problems simply require throwing ideas back and forth, and the scripting language controlling our cars is a key example. Keeping the design of this approachable enough for a passing student while also offering depth enough for a particularly enthusiastic programmer is a risky balance. Too far one way and the challenge is dull and lifeless, while too far the other way and suddenly you have textbook sized instruction manuals and about one willing participant. The critical importance of this design was not lost on us, so customer feedback was crucial to our development process. Naturally, user testing (as we'll discuss shortly in section 5.5) also played a strong role in this.

To put it in development terms, continuous feedback from our customer for our evaluation process is similar to the continuous integration approach we used in our design process. Having these short cycles of work followed by review allowed us to iterate toward the end game without losing our focus or direction. Maintaining strong communication with our customer was essential to managing our requirements and at the end of the day, making sure we were implementing something our personas were going to want to use. 

\section{Managing and Prioritising Requirements}
After confirming, or adding, requirements during our customer meeting, the next task was to plan how we would tackle the requirements. As mentioned in the section above, we tried to find out during our meetings which features were the highest priority for the customer. This was a good starting point for our internal prioritisation and planning. After a customer meeting we tried to meet as a group to prune our product backlog and estimate the size and priorities of our stories for the coming sprint (backlog refinement). You might expect that this is an easy process, as we had found out feature priorities directly from our customer, however this was not the case. With new features it is very difficult to estimate their difficulty. This affects their priority as the amount of time a task takes affects how achievable they are within a tight timescale. 

A very easy mistake to make would be to jump straight into implementing a new exciting feature that was mentioned in a customer meeting. We avoiding falling into this trap by strictly following the task prioritisation as detailed on our Trello scrum board. For example, before attempting to implement turbo boosts we had to ensure users could edit their AI scripts. Our team may have spent days on implementing turbo boosts, slowly realising that it's a lot more of a challenge than we initially thought. By the end of the sprint, only half of the new feature may have been built, and the important changes to script editing may not have been made. Script editing is unexciting compared to power-ups, but without it a user's experience could be made difficult and our own testing would be slowed down.

\subsection{Timeboxed Spikes}
One of the mechanisms that we used to tackle this problem was the use of timeboxed spikes. There were several occasions in our project where we had difficulty in estimating how long a task would take and thus been apprehensive to begin their implementation. With these tasks it was useful to put some time aside for someone to jump in and see how feasible it is to complete. For example, during our penultimate sprint one of our tasks was to upgrade our racing AI API from a raycast system to a spline-following system. This change was designed to improve the smoothness of the car's movement and allow for our API to be more easily expanded and simplified. However, we were unsure of how easy this change would be and what benefits it would provide. Therefore, we set aside one afternoon for a member of the team to look into how the change would be made and begin its implementation. We decided that if no decent progress or results had been achieved during this time then we would not continue with the task. Luckily for us, we managed to get one racetrack working with the new system fairly quickly and found out that it did provide many benefits. We then went on to add full support for the system and implemented it on other racetracks.

\subsection{Prototypes}
Another thing that we used to ensure that we were spending our time on the right things was the use of paper prototypes. When developing new front end systems or features it can be a great time saver to nail down what you want a UI to look like before you begin working on it. Otherwise, a first iteration could be built and then when it's shown to a stakeholder, the design might turn out to be sub-optimal. The UI may then have to be rebuilt from scratch, in the worst case. Paper prototypes can be used to minimise this wasted time by getting feedback before any time has been spent building it. For example, here is a paper prototype that we used for the script submission page (from the perspective of an anonymous user):

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{mockup.png}
\caption{Paper prototype of script submission screen.}
\end{figure}

In our case this was simply used among our team to decide on a design. Which, while not as useful as getting direct customer feedback, allowed us to save time on any later redesigns due to any differences in the imagined look of the site between team members. 

From our experience using UI prototyping software such as Fluid UI, it is rigid and takes only a little less time to create a prototype than drafting up an actual web page. Paper prototypes give the impression that it is a work in process, whereas realistic prototypes can give the impression that too much time and effort has been vested in a UI for it to be changed. We consequently opted for paper prototyping to ensure we received honest feedback from Ed.


\section{Automated Testing}

We integrated testing into our project lifecycle, ensuring that we tested often, and consequently kept the code in a releasable state. Automated testing helped us identify issues early and fix them immediately - this flexible response to change is key in agile software development, the development approach we have adopted. 

We used TeamCity\cite{teamcity} to manage continuous integration, including automated unit testing. To test the functionality of our website, we used two Node.s modules alongside the TeamCity Node plugin\cite{teamcitynode}: Zombie.js\cite{zombie} and Mocha\cite{mocha}. Zombie.js is a lightweight framework for client-side Javascript testing, and facilitates headless full-stack testing. In other words, Zombie.js let us programatically interact with a browser (without a graphical user interface). Mocha is also a Javascript testing framework, that simplifies test-driven and behaviour-driven development. Mocha provides an asynchronous test runner alongside interfaces such as {\tt describe, before, after, beforeEach} that simplify the declaration of test specifications.

Below is an example test for our login page followed by the corresponding section of the Mocha report. This demonstrates the readability of Mocha results, alongside the highly abstracted headless interactions that Zombie.js facilitates.  

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{zombietest.png}
\caption{Setup and example unit test for login page.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{mochareport.png}
\caption{Login page section of a Mocha report.}
\end{figure}

\section{Usability Testing}

Players have limited means of learning the API given the unconventional setting of a careers fair, so it was important that users would find the game intuitive and accessible. It was crucial to produce a API that was easy to use, and supported by helpful tutorials and example/starter scripts. 

One way of evaluating our project's usability was to evaluate the quality of the scripts produced by users. For example, we knew the tutorials or API needed improving if most students were submitting simple scripts (e.g. low constant speed scripts) or close variations of the example scripts. Beyond evaluating the quality of the scripts produced, we asked students to rate the quality of the API, tutorials and example scripts.

\subsection{Hallway Testing}

Hallway testing was our primary testing method since our target audience, Computing/Maths students, is too small to find reliable statistics through techniques such as the random sampling in A/B testing. Instead, we set up a mock G-Research careers fair stand with two workstations, and invited random students to write AI scripts.

We encouraged students to take around 5-10 minutes to write their scripts, as we believed this is representative of how long a student will have at a busy careers fair. As requested by our customer, we also provided handouts containing an API reference, short tutorial and space for making notes. Whilst a student waits for a turn on the laptop, this lets them think about the problem and potentially discuss it with other candidates or the recruiters. 

Although we did not prescribe a specific testing strategy, we had a few key measures:
\vspace*{-6mm}
\begin{enumerate} \itemsep -1pt 
\item Time spent by a student at the stand : We knew we would need to improve our project if all students spent five minutes on their scripts and left immediately afterwards. Instead, we hoped students would ask for more time, or stay to watch other scripts compete in tournaments.
\item Students interaction with those running the stand : Primarily this project's goal is to promote G-Research, so we wanted to observe whether the AI Racing Market was simply a fun distraction for students or a valuable means of connecting with prospective G-Research candidates.
\end{enumerate}

\subsubsection{Key Observations}

Here are some of the observations we made during hallway testing, and the ways in which we responded to improve our product : \\

\begin{tabularx}{\textwidth}{ | X | p{.70\textwidth} |}
\hline
Observation & Response \\
\hline\hline
Students would talk to one another, and not much with us 
	& It was clear to us that those running the stand had to be engaging, as students tended to play and simply talk to their friends. We expected this would change once actual recruiters were running the stand, yet passed on the feedback nonetheless. \\ \hline
After their turn, most students stayed to watch the tournament
	& This was a positive observation, that gave us the impression students were thinking competitively - instead of simply creating a script, getting frustrated with the product, and leaving in a huff. \\ \hline 
Users registered and headed straight to the edit screen 
	& We previously simplified login-flow to ensure users were not forced to enter personal details before submitting a script (there are fewer features available to anonymous users). However, the ability to edit existing scripts was necessary, as first attempts often weren't competitive or wouldn't work, so students happily registered with their Imperial email address to access this feature immediately. G-Research will like these email addresses. \\ \hline 
When reading the handout, students consistently questioned specific API functions
	& The first hand out we provided listed functions with minimal explanation - as developers, we were familiar with them and naively didn't document them sufficiently. We later adapted the hand out so that the majority of functions were thoroughly documented. \\ 
\hline
\end{tabularx}

\subsection{Collaborative Testing}

As stated on our value proposition canvas, the opportunity for students to collaborate with other students or G-Research employees when playing AI Racing market could be incredibly valuable. For example, a G-Research employee could use the opportunity to demonstrate how they work collaboratively on a daily basis - this is a more informative insight into how G-Research works than any dustbin-destined panflet.

To gauge whether collaboration worked with the AI Racing Market, we introduced individuals (of varying programming ability) to the game and pair-programmed to iteratively produce an advanced script. We did this with individuals of different programming ability levels, for example : a second-year Computing student (advanced), third-year Geophysics student (intermediate), and two secondary-school students (inexperienced). 

\subsubsection{Key Observations}

Here are some of the observations we made during collaborative testing, and the ways in which we responded to improve our product : \\

\begin{tabularx}{\textwidth}{ | X | X | p{.55\textwidth} |}
\hline
Student & Observation & Response\\
Ability Level & & \\
\hline\hline
Inexperienced 
& Unfamiliar with use of an API 
	& Alongside our standard API, we expanded our event-based API. This involves chaining calls similar to that of the builder pattern, in turn producing behaviour that is easy to read and understand, e.g. {\tt When.RaceStarts().ApiBoost()}. \\ \cline{2-3}
& Major difficulty debugging 
	& There is very little feedback between the Ace editor and Unity player itself, and it was infeasible to bridge this gap within our time limitations. Working collaboratively does improve the debugging process, as one individual can simply observe as the other drives.\\  \hline
Intermediate 
& Confusion over program scope 
	& Unlike the inexperienced users who simply related the main script body to a while-loop, a student questioned the construct and struggled to understand the game loop. We attempted to iron this kind of confusion out through strong documentation. \\ \cline{2-3}
& Struggle using advanced API 
	& Occasionally it was hard to produce an advanced script within a reasonable time frame. This was expected, as it is important the API caters for all ability levels whilst allowing advanced individuals to produce complex scripts.\\ \hline
Advanced 
& Lateral thinking 
	& We were pleasantly surprised by out-of-the-box thinking used by the advanced students. For example, before adding boost to our API ({\tt ApiBoost()}), a user was measuring game cycles to achieve a boost at the start of the race - similar to that seen in many computer games.  \\
\hline
\end{tabularx}

\section{Metrics}
Metrics were a useful part of the feedback process for two reasons. First, they allowed us to easily understand which parts of our workflow were going right and wrong, helping us improve working efficiency. Second, they allowed us to see if our product was working as intended, and recognise parts that needed changing - be it because they do not work or as part of general improvements.

\subsection{Workflow Metrics}
Our tasks on Trello had story points associated with them, as measures of task complexity. We followed these to gauge how much our team could complete in an average iteration, and see if there were any anomalies (which were often due to other coursework). In particular, we created burndown charts, allowing us to see whether we were going to complete our project on time or whether we would need to adjust our plan accordingly. 

\begin{figure}[h]
\makebox[\textwidth]
{
	\includegraphics[width=0.49\textwidth]{burndownold.png}
	\hfill    
	\includegraphics[width=0.49\textwidth]{burndown.png}
}
\caption{Burndown charts as project progressed to final sprint (right hand side)}
\end{figure}

Creating this graph initially revealed an alarming trend of an increase in the number of hours remaining (see left hand side of figure 4.1). However it was not an immediate cause for concern; this was due to extra stories being brought into scope after successful meetings with our customer, where we had exceed expectations and taken on stretch requirements. Reassuringly, our hours completed was fairly linear at this early stage; this helped us estimate how productive our team was each week ($\sim$10 story points) and simplify planning for the following sprints. As shown on the right hand side of figure 4.1, we eventually reached a final set of requirements and progressed at a similar working rate of towards the end of our final sprint. 

\subsection{Product Metrics}

Quantitative metrics can also provide an insight into product usage, although we did not use these extensively. However, we did identify some metrics that are easy to gather, which G-Research may find useful for product analysis following the product handover. In this section we outline both the metrics currently being collected, and how we used them, alongside a set of proposed metrics. 

\subsubsection{Race Performance Metrics}

In order to update our leaderboard, the final racing positions of each script are analysed and the performance rating of each script is updated (these are stored in each Script record on MongoDB). Beyond this, AI Racing Market could easily be extended to record how many times each script has raced, and information such as racing times and the number of times the car has flipped. This information could be stored temporally, in order to gauge how script performance improves over time as they are edited by the user. Ideally, we would expect users to see their scripts improve over time, and this may not be reflected by rank alone as competitor scripts may also be edited. 

\subsubsection{User Details}

As described previously in the User Interface Design section, we were asked to produce an administration panel (with restricted access) that lists each user, their university and degree details, alongside each of their scripts. Users do not have to upload any university and degree details, and can remove these details. Our client from G-Research did not outline how these details would be used. These could provide numerous insights into product usage. For example : we attempted to make our project accessible for all programming ability levels, however G-Research may find that mostly third and fourth year Computing students play AI Racing Market. This may indicate that less experienced developers, i.e. those from younger years or different courses, may find the prospect of competitive coding daunting. This would indicate flaws in our product design, that would need addressing.

\subsubsection{API Usage}

In the future, when plenty of scripts have been submitted, trends may emerge regarding API usage. This could be users not using the more advanced and challenging functionality, such as sharpness of corners (see {\tt api.GetNextCornerAmount()} in Appendix A.1), or users simply using the API object and ignoring the event API. These insights could be compared against script rating, and may indicate unbalanced, unusable or useless features of the API - it is clear that these features would need addressing. G-Research could gather data on API coverage at numerous stages of the script development lifecycle, such as on script submission or during script parsing.

\subsubsection{Website Usage Analysis}

In order to evaluate website usage, G-Research could use a tool such as Google Analytics \cite{googleanalytics}, examining usage and producing dashboards containing key performance indicators. Google Analytics would provide an insight into website events, for example time spent on the Unity web-player and how many times it gets reloaded; in-page analytics such as link tracking, for example whether users are accessing the login through the navigation bar button, or following a prompt elsewhere; and simple statistics such as exit rate (percentage of page views that were in a users last session) and bounce rate (percentage of single-page sessions).

We did not use tools like Google Analytics during development, as website visits from the development team would have inevitably skewed the statistics - we frequently re-visited specific pages that were being worked on intensively over short periods of time. We also decided not to use it during both hallway and collaborative testing, as we wouldn't have surveyed a sufficient user base to make any reliable statistical deductions on website properties. 


%We need to be able to evaluate how good a feature is, and quantitatively measure this so we can use it as feedback. There are a few methods for achieving this - for instance, for the website, we can record how long a user spends on a particular page, or how often they visit said page - this represents how popular the feature on the page is. 
%
%It is also useful to determine what exactly users want, and whether we have provided that for them or not. This is usually best implemented as a search function, where we can then see what users searched for against what page they ended up with; however, since this feature isn't required for our product, we will instead have to use more direct methods such as user surveys, or perhaps a direct feature request button.
%
%On this note, it would also be useful to determine which parts of the product the good candidates look at versus what parts everyone looks at, since this indicates what is interesting to the the type of people G-research want to hire, and means we can focus our development time on improving these parts in particular. For example, a strong candidate may take the time to fully understand the documentation, and would probably be able to improve on existing scripts easily.
%
%We also need to know how interested people are in general - in this case it can be useful to determine how long a user spends against how long they want to spend. For this, we'd need a ratio of the total time spent at the stall and how long the spend writing/racing AI scripts - perhaps they'd want some time to speak to a recruiter but the time spent writing a script was so long they didn't really have a chance, or wanted to move on to other things.
%
%The racing game is a good opportunity to record various statistics about the user. For instance, when starting the game, we can record which car models and racetracks are played the most, and how many players are commonly in one race. This is useful in deciding what the default settings should be, and also optimising the game for the most common amount of players.


